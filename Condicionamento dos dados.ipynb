{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d98d64-4871-458d-8792-03438fe1706c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. Imports\n",
    "# ============================================================\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, regexp_replace, when, trim,\n",
    "    min, max, count, lit\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Função: Normalizar nomes das colunas\n",
    "#    - Remove acentos\n",
    "#    - Converte para lowercase\n",
    "#    - Substitui caracteres especiais por \"_\"\n",
    "# ============================================================\n",
    "\n",
    "def normalizar_colunas(df: DataFrame) -> DataFrame:\n",
    "    novas_colunas = []\n",
    "\n",
    "    for c in df.columns:\n",
    "        c_norm = (\n",
    "            unicodedata.normalize(\"NFKD\", c)\n",
    "            .encode(\"ASCII\", \"ignore\")\n",
    "            .decode(\"utf-8\")\n",
    "        )\n",
    "        c_norm = c_norm.lower()\n",
    "        c_norm = re.sub(r\"[^a-z0-9]+\", \"_\", c_norm)\n",
    "        c_norm = c_norm.strip(\"_\")\n",
    "\n",
    "        novas_colunas.append(c_norm)\n",
    "\n",
    "    return df.toDF(*novas_colunas)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Leitura das tabelas INMET (2021–2025)\n",
    "# ============================================================\n",
    "\n",
    "tabelas = [\n",
    "    \"workspace.default.inmet_ne_se_a_409_aracaju_01_01_2021_a_31_12_2021\",\n",
    "    \"workspace.default.inmet_ne_se_a_409_aracaju_01_01_2022_a_31_12_2022\",\n",
    "    \"workspace.default.inmet_ne_se_a_409_aracaju_01_01_2023_a_31_12_2023\",\n",
    "    \"workspace.default.inmet_ne_se_a_409_aracaju_01_01_2024_a_31_12_2024\",\n",
    "    \"workspace.default.inmet_ne_se_a_409_aracaju_01_01_2025_a_31_08_2025\",\n",
    "]\n",
    "\n",
    "dfs_normalizados = [\n",
    "    normalizar_colunas(spark.table(tabela))\n",
    "    for tabela in tabelas\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Consolidação dos DataFrames\n",
    "#    - Union by name\n",
    "#    - Permite colunas ausentes\n",
    "# ============================================================\n",
    "\n",
    "df_consolidado = reduce(\n",
    "    lambda a, b: a.unionByName(b, allowMissingColumns=True),\n",
    "    dfs_normalizados\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Conversão de colunas numéricas\n",
    "#    - Trata valores inválidos\n",
    "#    - Converte vírgula para ponto\n",
    "#    - Cast para double\n",
    "# ============================================================\n",
    "\n",
    "colunas_nao_numericas = {\"data\", \"hora_utc\"}\n",
    "\n",
    "def converter_para_double(df: DataFrame) -> DataFrame:\n",
    "    df_out = df\n",
    "\n",
    "    for c in df.columns:\n",
    "        if c not in colunas_nao_numericas:\n",
    "            df_out = df_out.withColumn(\n",
    "                c,\n",
    "                when(\n",
    "                    col(c).isNull()\n",
    "                    | (trim(col(c).cast(\"string\")) == \"\")\n",
    "                    | (col(c).cast(\"string\").isin(\"-9999\", \"////\", \"NaN\")),\n",
    "                    None\n",
    "                ).otherwise(\n",
    "                    regexp_replace(\n",
    "                        col(c).cast(\"string\"), \",\", \".\"\n",
    "                    ).cast(\"double\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "df_numerico = converter_para_double(df_consolidado)\n",
    "\n",
    "# Verificação do schema final\n",
    "df_numerico.printSchema()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Identificação das colunas numéricas\n",
    "# ============================================================\n",
    "\n",
    "colunas_numericas = [\n",
    "    c for c, t in df_numerico.dtypes\n",
    "    if t not in (\"string\", \"date\")\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. Análise de qualidade dos dados\n",
    "#    - Mínimo\n",
    "#    - Máximo\n",
    "#    - Dados faltantes\n",
    "# ============================================================\n",
    "\n",
    "total_registros = df_numerico.count()\n",
    "\n",
    "dfs_stats = []\n",
    "\n",
    "for c in colunas_numericas:\n",
    "    df_stats = df_numerico.select(\n",
    "        lit(c).alias(\"coluna\"),\n",
    "        min(c).alias(\"minimo\"),\n",
    "        max(c).alias(\"maximo\"),\n",
    "        (lit(total_registros) - count(c)).alias(\"dados_faltantes\"),\n",
    "        lit(total_registros).alias(\"total_registros\")\n",
    "    )\n",
    "    dfs_stats.append(df_stats)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. Consolidação da tabela de qualidade\n",
    "# ============================================================\n",
    "\n",
    "df_qualidade = reduce(\n",
    "    lambda a, b: a.unionByName(b),\n",
    "    dfs_stats\n",
    ")\n",
    "\n",
    "display(df_qualidade)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Condicionamento dos dados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
