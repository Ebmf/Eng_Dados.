<<<<<< V2
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d98d64-4871-458d-8792-03438fe1706c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766186685157}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. Imports\n",
    "# ============================================================\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, regexp_replace, when, trim,\n",
    "    min, max, count, lit, coalesce\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Função: Normalizar nomes das colunas\n",
    "# ============================================================\n",
    "\n",
    "def normalizar_colunas(df: DataFrame) -> DataFrame:\n",
    "    novas_colunas = []\n",
    "\n",
    "    for c in df.columns:\n",
    "        c_norm = (\n",
    "            unicodedata.normalize(\"NFKD\", c)\n",
    "            .encode(\"ASCII\", \"ignore\")\n",
    "            .decode(\"utf-8\")\n",
    "        )\n",
    "        c_norm = c_norm.lower()\n",
    "        c_norm = re.sub(r\"[^a-z0-9]+\", \"_\", c_norm)\n",
    "        c_norm = c_norm.strip(\"_\")\n",
    "\n",
    "        novas_colunas.append(c_norm)\n",
    "\n",
    "    return df.toDF(*novas_colunas)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Leitura das tabelas INMET (2021–2025)\n",
    "# ============================================================\n",
    "\n",
    "tabelas = [\n",
    "    \"workspace.default.inmet_ne_se_a_409_aracaju_01_01_2021_a_31_12_2021\",\n",
    "    \"workspace.default.inmet_ne_se_a_409_aracaju_01_01_2022_a_31_12_2022\",\n",
    "    \"workspace.default.inmet_ne_se_a_409_aracaju_01_01_2023_a_31_12_2023\",\n",
    "    \"workspace.default.inmet_ne_se_a_409_aracaju_01_01_2024_a_31_12_2024\",\n",
    "    \"workspace.default.inmet_ne_se_a_409_aracaju_01_01_2025_a_31_08_2025\",\n",
    "]\n",
    "\n",
    "dfs_normalizados = [\n",
    "    normalizar_colunas(spark.table(t))\n",
    "    for t in tabelas\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Consolidação dos DataFrames\n",
    "# ============================================================\n",
    "\n",
    "df_consolidado = reduce(\n",
    "    lambda a, b: a.unionByName(b, allowMissingColumns=True),\n",
    "    dfs_normalizados\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Conversão de colunas numéricas\n",
    "# ============================================================\n",
    "\n",
    "colunas_nao_numericas = {\"data\", \"hora_utc\"}\n",
    "\n",
    "def converter_para_double(df: DataFrame) -> DataFrame:\n",
    "    df_out = df\n",
    "\n",
    "    for c in df.columns:\n",
    "        if c not in colunas_nao_numericas:\n",
    "            df_out = df_out.withColumn(\n",
    "                c,\n",
    "                when(\n",
    "                    col(c).isNull()\n",
    "                    | (trim(col(c).cast(\"string\")) == \"\")\n",
    "                    | (col(c).cast(\"string\").isin(\"-9999\", \"////\", \"NaN\")),\n",
    "                    None\n",
    "                ).otherwise(\n",
    "                    regexp_replace(\n",
    "                        col(c).cast(\"string\"), \",\", \".\"\n",
    "                    ).cast(\"double\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "df_numerico = converter_para_double(df_consolidado)\n",
    "\n",
    "df_numerico.printSchema()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Adequação / Harmonização de colunas equivalentes\n",
    "#    - Une colunas semanticamente iguais\n",
    "#    - Prioriza valores não nulos\n",
    "# ============================================================\n",
    "\n",
    "mapa_colunas_equivalentes = {\n",
    "    \"precipitacao_total_mm\": [\n",
    "        \"precipitacao_total_horario_mm\",\n",
    "        \"precipitacao_total_mm\",\"precipitao_total_horrio_mm\",\"precipitao_total_mm\"\n",
    "    ],\n",
    "    \"temperatura_ar_c\": [\n",
    "        \"temperatura_do_ar_bulbo_seco_horaria_c\",\n",
    "        \"temperatura_do_ar_bulbo_seco_c\"\n",
    "    ],\n",
    "    \"umidade_relativa_ar\": [\n",
    "        \"umidade_relativa_do_ar\",\n",
    "        \"umidade_relativa_do_ar_horaria\"\n",
    "    ],\n",
    "    \"pressao_atmosferica_mb\": [\n",
    "        \"pressao_atmosferica_ao_nivel_da_estacao_mb\",\n",
    "        \"pressao_atmosferica_ao_nivel_da_estacao_horaria_mb\"\n",
    "    ],\n",
    "    \"vento_direcao_gr\": [\n",
    "        \"vento_direo_horaria_gr_gr\",\n",
    "        \"vento\",\"vento_direcao_gr\"\n",
    "    ],\n",
    "    \"vento_rajada_maxima_m_s\": [\n",
    "        \"vento_maxima_m_s\",\n",
    "        \"vento_rajada_maxima_m_s\"\n",
    "    ],\n",
    "    \"vento_velocidade_m_s\": [\n",
    "        \"vento_velocidade_horaria_m_s\",\n",
    "        \"vento_velocidade_m_s\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "for coluna_final, colunas_origem in mapa_colunas_equivalentes.items():\n",
    "    colunas_existentes = [\n",
    "        col(c) for c in colunas_origem if c in df_numerico.columns\n",
    "    ]\n",
    "\n",
    "    if colunas_existentes:\n",
    "        df_numerico = df_numerico.withColumn(\n",
    "            coluna_final,\n",
    "            coalesce(*colunas_existentes)\n",
    "        ).drop(*[c for c in colunas_origem if c in df_numerico.columns])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. Identificação das colunas numéricas\n",
    "# ============================================================\n",
    "\n",
    "colunas_numericas = [\n",
    "    c for c, t in df_numerico.dtypes\n",
    "    if t not in (\"string\", \"date\")\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. Análise de qualidade dos dados\n",
    "# ============================================================\n",
    "\n",
    "total_registros = df_numerico.count()\n",
    "\n",
    "dfs_stats = []\n",
    "\n",
    "for c in colunas_numericas:\n",
    "    df_stats = df_numerico.select(\n",
    "        lit(c).alias(\"coluna\"),\n",
    "        min(c).alias(\"minimo\"),\n",
    "        max(c).alias(\"maximo\"),\n",
    "        (lit(total_registros) - count(c)).alias(\"dados_faltantes\"),\n",
    "        lit(total_registros).alias(\"total_registros\")\n",
    "    )\n",
    "    dfs_stats.append(df_stats)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. Consolidação da tabela de qualidade\n",
    "# ============================================================\n",
    "\n",
    "df_qualidade = reduce(\n",
    "    lambda a, b: a.unionByName(b),\n",
    "    dfs_stats\n",
    ")\n",
    "\n",
    "display(df_qualidade)\n",
    "# ============================================================\n",
    "# 10. Ranking dos extremos climáticos\n",
    "# ============================================================\n",
    "\n",
    "# 10.1. 10 dias mais chuvosos\n",
    "df_chuva_dia = df_numerico.groupBy(\"data\").sum(\"precipitacao_total_mm\") \\\n",
    "    .withColumnRenamed(\"sum(precipitacao_total_mm)\", \"precipitacao_total_dia_mm\") \\\n",
    "    .orderBy(col(\"precipitacao_total_dia_mm\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "display(df_chuva_dia)\n",
    "\n",
    "# 10.2. 10 dias mais quentes\n",
    "df_temp_dia = df_numerico.groupBy(\"data\").avg(\"temperatura_ar_c\") \\\n",
    "    .withColumnRenamed(\"avg(temperatura_ar_c)\", \"temperatura_media_dia_c\") \\\n",
    "    .orderBy(col(\"temperatura_media_dia_c\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "display(df_temp_dia)\n",
    "\n",
    "# 10.3. 10 dias mais frios\n",
    "df_temp_fria_dia = (\n",
    "    df_numerico\n",
    "    .groupBy(\"data\")\n",
    "    .avg(\"temperatura_ar_c\")\n",
    "    .withColumnRenamed(\"avg(temperatura_ar_c)\", \"temperatura_media_dia_c\")\n",
    "    .filter(col(\"temperatura_media_dia_c\").isNotNull())  # remove valores nulos\n",
    "    .orderBy(col(\"temperatura_media_dia_c\").asc())\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "display(df_temp_fria_dia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f6fadc3-f0f9-4ca7-b54a-b115c3d25a2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Condicionamento dos dados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
=======
# ============================================================
# 1. Imports
# ============================================================

import unicodedata
import re
from functools import reduce

from pyspark.sql import DataFrame
from pyspark.sql.functions import (
    col, regexp_replace, when, trim,
    min, max, count, lit, coalesce
)


# ============================================================
# 2. Função: Normalizar nomes das colunas
# ============================================================

def normalizar_colunas(df: DataFrame) -> DataFrame:
    novas_colunas = []

    for c in df.columns:
        c_norm = (
            unicodedata.normalize("NFKD", c)
            .encode("ASCII", "ignore")
            .decode("utf-8")
        )
        c_norm = c_norm.lower()
        c_norm = re.sub(r"[^a-z0-9]+", "_", c_norm)
        c_norm = c_norm.strip("_")

        novas_colunas.append(c_norm)

    return df.toDF(*novas_colunas)


# ============================================================
# 3. Leitura das tabelas INMET (2021–2025)
# ============================================================

tabelas = [
    "workspace.default.inmet_ne_se_a_409_aracaju_01_01_2021_a_31_12_2021",
    "workspace.default.inmet_ne_se_a_409_aracaju_01_01_2022_a_31_12_2022",
    "workspace.default.inmet_ne_se_a_409_aracaju_01_01_2023_a_31_12_2023",
    "workspace.default.inmet_ne_se_a_409_aracaju_01_01_2024_a_31_12_2024",
    "workspace.default.inmet_ne_se_a_409_aracaju_01_01_2025_a_31_08_2025",
]

dfs_normalizados = [
    normalizar_colunas(spark.table(t))
    for t in tabelas
]


# ============================================================
# 4. Consolidação dos DataFrames
# ============================================================

df_consolidado = reduce(
    lambda a, b: a.unionByName(b, allowMissingColumns=True),
    dfs_normalizados
)


# ============================================================
# 5. Conversão de colunas numéricas
# ============================================================

colunas_nao_numericas = {"data", "hora_utc"}

def converter_para_double(df: DataFrame) -> DataFrame:
    df_out = df

    for c in df.columns:
        if c not in colunas_nao_numericas:
            df_out = df_out.withColumn(
                c,
                when(
                    col(c).isNull()
                    | (trim(col(c).cast("string")) == "")
                    | (col(c).cast("string").isin("-9999", "////", "NaN")),
                    None
                ).otherwise(
                    regexp_replace(
                        col(c).cast("string"), ",", "."
                    ).cast("double")
                )
            )

    return df_out


df_numerico = converter_para_double(df_consolidado)

df_numerico.printSchema()


# ============================================================
# 6. Adequação / Harmonização de colunas equivalentes
#    - Une colunas semanticamente iguais
#    - Prioriza valores não nulos
# ============================================================

mapa_colunas_equivalentes = {
    "precipitacao_total_mm": [
        "precipitacao_total_horario_mm",
        "precipitacao_total_mm","precipitao_total_horrio_mm","precipitao_total_mm"
    ],
    "temperatura_ar_c": [
        "temperatura_do_ar_bulbo_seco_horaria_c",
        "temperatura_do_ar_bulbo_seco_c"
    ],
    "umidade_relativa_ar": [
        "umidade_relativa_do_ar",
        "umidade_relativa_do_ar_horaria"
    ],
    "pressao_atmosferica_mb": [
        "pressao_atmosferica_ao_nivel_da_estacao_mb",
        "pressao_atmosferica_ao_nivel_da_estacao_horaria_mb"
    ],
    "vento_direcao_gr": [
        "vento_direo_horaria_gr_gr",
        "vento","vento_direcao_gr"
    ],
    "vento_rajada_maxima_m_s": [
        "vento_maxima_m_s",
        "vento_rajada_maxima_m_s"
    ],
    "vento_velocidade_m_s": [
        "vento_velocidade_horaria_m_s",
        "vento_velocidade_m_s"
    ],
 >>>>>>> main
}

for coluna_final, colunas_origem in mapa_colunas_equivalentes.items():
    colunas_existentes = [
        col(c) for c in colunas_origem if c in df_numerico.columns
    ]

    if colunas_existentes:
        df_numerico = df_numerico.withColumn(
            coluna_final,
            coalesce(*colunas_existentes)
        ).drop(*[c for c in colunas_origem if c in df_numerico.columns])


# ============================================================
# 7. Identificação das colunas numéricas
# ============================================================

colunas_numericas = [
    c for c, t in df_numerico.dtypes
    if t not in ("string", "date")
]


# ============================================================
# 8. Análise de qualidade dos dados
# ============================================================

total_registros = df_numerico.count()

dfs_stats = []

for c in colunas_numericas:
    df_stats = df_numerico.select(
        lit(c).alias("coluna"),
        min(c).alias("minimo"),
        max(c).alias("maximo"),
        (lit(total_registros) - count(c)).alias("dados_faltantes"),
        lit(total_registros).alias("total_registros")
    )
    dfs_stats.append(df_stats)


# ============================================================
# 9. Consolidação da tabela de qualidade
# ============================================================

df_qualidade = reduce(
    lambda a, b: a.unionByName(b),
    dfs_stats
)

display(df_qualidade)
# ============================================================
# 10. Ranking dos extremos climáticos
# ============================================================

# 10.1. 10 dias mais chuvosos
df_chuva_dia = df_numerico.groupBy("data").sum("precipitacao_total_mm") \
    .withColumnRenamed("sum(precipitacao_total_mm)", "precipitacao_total_dia_mm") \
    .orderBy(col("precipitacao_total_dia_mm").desc()) \
    .limit(10)

display(df_chuva_dia)

# 10.2. 10 dias mais quentes
df_temp_dia = df_numerico.groupBy("data").avg("temperatura_ar_c") \
    .withColumnRenamed("avg(temperatura_ar_c)", "temperatura_media_dia_c") \
    .orderBy(col("temperatura_media_dia_c").desc()) \
    .limit(10)

display(df_temp_dia)

# 10.3. 10 dias mais frios
df_temp_fria_dia = (
    df_numerico
    .groupBy("data")
    .avg("temperatura_ar_c")
    .withColumnRenamed("avg(temperatura_ar_c)", "temperatura_media_dia_c")
    .filter(col("temperatura_media_dia_c").isNotNull())  # remove valores nulos
    .orderBy(col("temperatura_media_dia_c").asc())
    .limit(10)
)

display(df_temp_fria_dia)
