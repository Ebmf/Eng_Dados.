{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffcfb021-1200-46b5-b621-12043b078f78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. Imports\n",
    "# ============================================================\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, regexp_replace, when, trim,\n",
    "    min, max, count, lit, coalesce\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Função: Normalizar nomes das colunas\n",
    "# ============================================================\n",
    "\n",
    "def normalizar_colunas(df: DataFrame) -> DataFrame:\n",
    "    novas_colunas = []\n",
    "\n",
    "    for c in df.columns:\n",
    "        c_norm = (\n",
    "            unicodedata.normalize(\"NFKD\", c)\n",
    "            .encode(\"ASCII\", \"ignore\")\n",
    "            .decode(\"utf-8\")\n",
    "        )\n",
    "        c_norm = c_norm.lower()\n",
    "        c_norm = re.sub(r\"[^a-z0-9]+\", \"_\", c_norm)\n",
    "        c_norm = c_norm.strip(\"_\")\n",
    "\n",
    "        novas_colunas.append(c_norm)\n",
    "\n",
    "    return df.toDF(*novas_colunas)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Leitura das tabelas INMET (2021–2025)\n",
    "# ============================================================\n",
    "\n",
    "tabelas = [\n",
    "    \"workspace.default.inmet_ne_se_a_409_aracaju_01_01_2021_a_31_12_2021\",\n",
    "    \"workspace.default.inmet_ne_se_a_409_aracaju_01_01_2022_a_31_12_2022\",\n",
    "    \"workspace.default.inmet_ne_se_a_409_aracaju_01_01_2023_a_31_12_2023\",\n",
    "    \"workspace.default.inmet_ne_se_a_409_aracaju_01_01_2024_a_31_12_2024\",\n",
    "    \"workspace.default.inmet_ne_se_a_409_aracaju_01_01_2025_a_31_08_2025\",\n",
    "]\n",
    "\n",
    "dfs_normalizados = [\n",
    "    normalizar_colunas(spark.table(t))\n",
    "    for t in tabelas\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Consolidação dos DataFrames\n",
    "# ============================================================\n",
    "\n",
    "df_consolidado = reduce(\n",
    "    lambda a, b: a.unionByName(b, allowMissingColumns=True),\n",
    "    dfs_normalizados\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Conversão de colunas numéricas\n",
    "# ============================================================\n",
    "\n",
    "colunas_nao_numericas = {\"data\", \"hora_utc\"}\n",
    "\n",
    "def converter_para_double(df: DataFrame) -> DataFrame:\n",
    "    df_out = df\n",
    "\n",
    "    for c in df.columns:\n",
    "        if c not in colunas_nao_numericas:\n",
    "            df_out = df_out.withColumn(\n",
    "                c,\n",
    "                when(\n",
    "                    col(c).isNull()\n",
    "                    | (trim(col(c).cast(\"string\")) == \"\")\n",
    "                    | (col(c).cast(\"string\").isin(\"-9999\", \"////\", \"NaN\")),\n",
    "                    None\n",
    "                ).otherwise(\n",
    "                    regexp_replace(\n",
    "                        col(c).cast(\"string\"), \",\", \".\"\n",
    "                    ).cast(\"double\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "df_numerico = converter_para_double(df_consolidado)\n",
    "\n",
    "df_numerico.printSchema()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Adequação / Harmonização de colunas equivalentes\n",
    "#    - Une colunas semanticamente iguais\n",
    "#    - Prioriza valores não nulos\n",
    "# ============================================================\n",
    "\n",
    "mapa_colunas_equivalentes = {\n",
    "    \"precipitacao_total_mm\": [\n",
    "        \"precipitacao_total_horario_mm\",\n",
    "        \"precipitacao_total_mm\",\"precipitao_total_horrio_mm\",\"precipitao_total_mm\"\n",
    "    ],\n",
    "    \"temperatura_ar_c\": [\n",
    "        \"temperatura_do_ar_bulbo_seco_horaria_c\",\n",
    "        \"temperatura_do_ar_bulbo_seco_c\"\n",
    "    ],\n",
    "    \"umidade_relativa_ar\": [\n",
    "        \"umidade_relativa_do_ar\",\n",
    "        \"umidade_relativa_do_ar_horaria\"\n",
    "    ],\n",
    "    \"pressao_atmosferica_mb\": [\n",
    "        \"pressao_atmosferica_ao_nivel_da_estacao_mb\",\n",
    "        \"pressao_atmosferica_ao_nivel_da_estacao_horaria_mb\"\n",
    "    ],\n",
    "    \"vento_direcao_gr\": [\n",
    "        \"vento_direo_horaria_gr_gr\",\n",
    "        \"vento\",\"vento_direcao_gr\"\n",
    "    ],\n",
    "    \"vento_rajada_maxima_m_s\": [\n",
    "        \"vento_maxima_m_s\",\n",
    "        \"vento_rajada_maxima_m_s\"\n",
    "    ],\n",
    "    \"vento_velocidade_m_s\": [\n",
    "        \"vento_velocidade_horaria_m_s\",\n",
    "        \"vento_velocidade_m_s\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "for coluna_final, colunas_origem in mapa_colunas_equivalentes.items():\n",
    "    colunas_existentes = [\n",
    "        col(c) for c in colunas_origem if c in df_numerico.columns\n",
    "    ]\n",
    "\n",
    "    if colunas_existentes:\n",
    "        df_numerico = df_numerico.withColumn(\n",
    "            coluna_final,\n",
    "            coalesce(*colunas_existentes)\n",
    "        ).drop(*[c for c in colunas_origem if c in df_numerico.columns])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. Identificação das colunas numéricas\n",
    "# ============================================================\n",
    "\n",
    "colunas_numericas = [\n",
    "    c for c, t in df_numerico.dtypes\n",
    "    if t not in (\"string\", \"date\")\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. Análise de qualidade dos dados\n",
    "# ============================================================\n",
    "\n",
    "total_registros = df_numerico.count()\n",
    "\n",
    "dfs_stats = []\n",
    "\n",
    "for c in colunas_numericas:\n",
    "    df_stats = df_numerico.select(\n",
    "        lit(c).alias(\"coluna\"),\n",
    "        min(c).alias(\"minimo\"),\n",
    "        max(c).alias(\"maximo\"),\n",
    "        (lit(total_registros) - count(c)).alias(\"dados_faltantes\"),\n",
    "        lit(total_registros).alias(\"total_registros\")\n",
    "    )\n",
    "    dfs_stats.append(df_stats)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. Consolidação da tabela de qualidade\n",
    "# ============================================================\n",
    "\n",
    "df_qualidade = reduce(\n",
    "    lambda a, b: a.unionByName(b),\n",
    "    dfs_stats\n",
    ")\n",
    "\n",
    "display(df_qualidade)\n",
    "# ============================================================\n",
    "# 10. Ranking dos extremos climáticos\n",
    "# ============================================================\n",
    "\n",
    "# 10.1. 10 dias mais chuvosos\n",
    "df_chuva_dia = df_numerico.groupBy(\"data\").sum(\"precipitacao_total_mm\") \\\n",
    "    .withColumnRenamed(\"sum(precipitacao_total_mm)\", \"precipitacao_total_dia_mm\") \\\n",
    "    .orderBy(col(\"precipitacao_total_dia_mm\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "display(df_chuva_dia)\n",
    "\n",
    "# 10.2. 10 dias mais quentes\n",
    "df_temp_dia = df_numerico.groupBy(\"data\").avg(\"temperatura_ar_c\") \\\n",
    "    .withColumnRenamed(\"avg(temperatura_ar_c)\", \"temperatura_media_dia_c\") \\\n",
    "    .orderBy(col(\"temperatura_media_dia_c\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "display(df_temp_dia)\n",
    "\n",
    "# 10.3. 10 dias mais frios\n",
    "df_temp_fria_dia = (\n",
    "    df_numerico\n",
    "    .groupBy(\"data\")\n",
    "    .avg(\"temperatura_ar_c\")\n",
    "    .withColumnRenamed(\"avg(temperatura_ar_c)\", \"temperatura_media_dia_c\")\n",
    "    .filter(col(\"temperatura_media_dia_c\").isNotNull())  # remove valores nulos\n",
    "    .orderBy(col(\"temperatura_media_dia_c\").asc())\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "display(df_temp_fria_dia)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Condicionamento",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
